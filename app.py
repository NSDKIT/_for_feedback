import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import chi2_contingency
import networkx as nx
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import re
from collections import Counter
import itertools
# import openai

# # OpenAI APIã‚­ãƒ¼ã®è¨­å®š
# try:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
# except Exception as e:
#     st.error(f"OpenAI APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
#     st.stop()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

def preprocess_text(text_series):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
    # NaNã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    text_series = text_series.fillna('')
    
    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
    text_series = text_series.str.translate(str.maketrans(
        'ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
    ))
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text_series = text_series.str.replace(r'[^\w\s]', '', regex=True)
    
    return text_series

def extract_themes(text_series, n_themes=5):
    """ãƒ†ãƒ¼ãƒã®æŠ½å‡º"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = ' '.join(text_series).split()
    
    # å˜èªã®å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(words)
    
    # ä¸Šä½n_themeså€‹ã®ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    themes = [word for word, count in word_counts.most_common(n_themes)]
    
    return themes

def build_co_occurrence_network(text_series, window_size=2):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = ' '.join(text_series).split()
    
    # å…±èµ·é–¢ä¿‚ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    co_occurrence = {}
    for i in range(len(words)):
        for j in range(i+1, min(i+window_size+1, len(words))):
            pair = tuple(sorted([words[i], words[j]]))
            co_occurrence[pair] = co_occurrence.get(pair, 0) + 1
    
    return co_occurrence

def analyze_attributes(df, attributes):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æï¼ˆé€šå¸¸å±æ€§ï¼‹è¤‡æ•°å›ç­”å±æ€§ã®é †ä½åˆ†å¸ƒï¼‰"""
    stats = {}
    ranked_distributions = {}
    for attr in attributes:
        if attr in MULTIPLE_CHOICE_QUESTIONS:
            # è¤‡æ•°å›ç­”å±æ€§ã¯é †ä½ã”ã¨ã®åˆ†å¸ƒã‚’è¨ˆç®—
            ranked_distributions[attr] = process_ranked_attributes(df, attr)
        else:
            value_counts = df[attr].value_counts()
            stats[attr] = {
                'count': df[attr].count(),
                'unique': df[attr].nunique(),
                'top': value_counts.index[0] if not value_counts.empty else None,
                'freq': value_counts.iloc[0] if not value_counts.empty else 0,
                'distribution': value_counts.to_dict()
            }
    # ã‚¯ãƒ­ã‚¹é›†è¨ˆ
    cross_tabs = {}
    for attr1, attr2 in itertools.combinations(attributes, 2):
        cross_tabs[f"{attr1}_vs_{attr2}"] = pd.crosstab(df[attr1], df[attr2])
    return stats, cross_tabs, ranked_distributions

def process_ranked_attributes(df, question):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ç”¨ï¼šé †ä½ä»˜ãè¤‡æ•°å›ç­”ã®å‡¦ç†ï¼ˆå…¨ä½“åˆ†å¸ƒã‚‚è¿”ã™ï¼‰"""
    answers = []
    for response in df[question]:
        if pd.isna(response):
            continue
        items = [item.strip() for item in str(response).split('ã€')]
        for rank, item in enumerate(items, 1):
            answers.append((item, rank))
    if not answers:
        return {}
    result_df = pd.DataFrame(answers, columns=['å›ç­”', 'é †ä½'])
    rank_distributions = {}
    max_rank = result_df['é †ä½'].max()
    # å„é †ä½ã”ã¨ã®åˆ†å¸ƒ
    for rank in range(1, max_rank + 1):
        rank_answers = result_df[result_df['é †ä½'] == rank]['å›ç­”']
        if not rank_answers.empty:
            rank_distributions[f"{rank}ä½"] = rank_answers.value_counts()
    # å…¨ä½“åˆ†å¸ƒ
    all_counts = result_df['å›ç­”'].value_counts()
    rank_distributions['å…¨ä½“'] = all_counts
    return rank_distributions

# è¤‡æ•°å›ç­”ã®è³ªå•ãƒªã‚¹ãƒˆ
MULTIPLE_CHOICE_QUESTIONS = [
    "â–ªï¸ ä¼æ¥­ã‚’é¸ã¶éš›ã«é‡è¦–ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆ",
    "â–ªï¸ ç”Ÿãç”Ÿãåƒã„ã¦ã„ã‚‹ã¨æ„Ÿã˜ã‚‹çŠ¶æ…‹",
    "â–ªï¸ åƒããŒã„ã‚’æ„Ÿã˜ã‚‹ã¨ã",
    "â–ªï¸ å°±æ´»æƒ…å ±æº"
]

def analyze_yes_no_questions(df, yes_no_questions, attributes):
    """2æŠè³ªå•ã®åˆ†æ"""
    # 1. å›ç­”åˆ†å¸ƒã®é›†è¨ˆ
    response_dist = {}
    for question in yes_no_questions:
        if question in MULTIPLE_CHOICE_QUESTIONS:
            # è¤‡æ•°å›ç­”ã®è³ªå•ã¯ç‰¹åˆ¥å‡¦ç†
            ranked_answers = process_ranked_attributes(df, question)
            if not ranked_answers:
                response_dist[question] = {}
            else:
                response_dist[question] = ranked_answers
        else:
            # é€šå¸¸ã®2æŠè³ªå•
            response_dist[question] = df[question].value_counts(normalize=True)
    
    # 2. å±æ€§åˆ¥ã®å‚¾å‘åˆ†æ
    trend_analysis = {}
    for question in yes_no_questions:
        for attribute in attributes:
            trend_analysis[f"{question}_by_{attribute}"] = pd.crosstab(
                df[attribute], 
                df[question], 
                normalize='index'
            )
    
    # 3. ã‚«ã‚¤äºŒä¹—æ¤œå®š
    chi2_results = {}
    for question in yes_no_questions:
        for attribute in attributes:
            contingency_table = pd.crosstab(df[attribute], df[question])
            chi2, p, dof, expected = chi2_contingency(contingency_table)
            chi2_results[f"{question}_vs_{attribute}"] = {
                'chi2': chi2,
                'p_value': p,
                'dof': dof
            }
    
    return response_dist, trend_analysis, chi2_results

def analyze_free_text(df, text_columns):
    """è‡ªç”±è¨˜è¿°ã®åˆ†æ"""
    results = {}
    
    for column in text_columns:
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_text = preprocess_text(df[column])
        
        # ãƒ†ãƒ¼ãƒã®æŠ½å‡º
        themes = extract_themes(processed_text)
        
        # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰
        co_occurrence = build_co_occurrence_network(processed_text)
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            font_path=None  # ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
        ).generate(' '.join(processed_text))
        
        results[column] = {
            'themes': themes,
            'co_occurrence': co_occurrence,
            'wordcloud': wordcloud
        }
    
    return results

def visualize_analysis(df, attributes, yes_no_questions, text_columns):
    """åˆ†æçµæœã®å¯è¦–åŒ–"""
    # å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
    stats, cross_tabs, attribute_ranked = analyze_attributes(df, attributes)
    # 2æŠè³ªå•ã®åˆ†æ
    response_dist, trend_analysis, chi2_results = analyze_yes_no_questions(
        df, yes_no_questions, attributes
    )
    # è‡ªç”±è¨˜è¿°ã®åˆ†æ
    text_analysis = analyze_free_text(df, text_columns)
    return {
        'stats': stats,
        'cross_tabs': cross_tabs,
        'attribute_ranked': attribute_ranked,
        'response_dist': response_dist,
        'trend_analysis': trend_analysis,
        'chi2_results': chi2_results,
        'text_analysis': text_analysis
    }

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

if uploaded_file is not None:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)
    
    # è³ªå•ã®åˆ†é¡
    attributes = [col for col in df.columns if 'â–ªï¸' in col]
    yes_no_questions = [col for col in df.columns if 'âš«ï¸' in col]
    text_columns = [col for col in df.columns if 'ãƒ»' in col]
    
    # åˆ†æã®å®Ÿè¡Œ
    analysis_results = visualize_analysis(df, attributes, yes_no_questions, text_columns)
    
    # ã‚¿ãƒ–ã®ä½œæˆ
    tab_attributes, tab_yes_no, tab_text, tab_summary = st.tabs([
        "1. å±æ€§åˆ†æ",
        "2. 2æŠè³ªå•åˆ†æ",
        "3. è‡ªç”±è¨˜è¿°åˆ†æ",
        "4. ç·åˆåˆ†æ"
    ])
    
    # 1. å±æ€§åˆ†æã‚¿ãƒ–
    with tab_attributes:
        st.markdown("### 1. å±æ€§åˆ†æ")
        subtab_dist, subtab_cross = st.tabs(["å±æ€§ã”ã¨ã®åˆ†å¸ƒï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰", "ã‚¯ãƒ­ã‚¹é›†è¨ˆ"])
        with subtab_dist:
            # 3åˆ—ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ
            cols = st.columns(3)
            col_index = 0
            
            for attr in attributes:
                if attr in MULTIPLE_CHOICE_QUESTIONS:
                    # è¤‡æ•°å›ç­”ã®è³ªå•ã¯é †ä½ã”ã¨ã«ç‹¬ç«‹ã—ãŸå›³ã‚’è¡¨ç¤º
                    rank_distributions = analysis_results['attribute_ranked'].get(attr, {})
                    rank_keys = [k for k in rank_distributions.keys() if k != 'å…¨ä½“']
                    
                    for rank in sorted(rank_keys, key=lambda x: int(x.replace('ä½','')) if x.endswith('ä½') else 999):
                        with cols[col_index % 3]:
                            rank_dist = rank_distributions[rank]
                            st.markdown(f"###### {attr} - {rank}")
                            fig = px.pie(
                                values=rank_dist.values,
                                names=rank_dist.index,
                                width=400,
                                height=400
                            )
                            fig.update_layout(
                                uniformtext_minsize=12,
                                uniformtext_mode='hide'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        col_index += 1
                else:
                    # é€šå¸¸ã®å±æ€§ã¯1ã¤ã®å›³ã‚’è¡¨ç¤º
                    with cols[col_index % 3]:
                        stat = analysis_results['stats'][attr]
                        st.markdown(f"###### {attr}")
                        fig = px.pie(
                            values=list(stat['distribution'].values()),
                            names=list(stat['distribution'].keys()),
                            width=400,
                            height=400
                        )
                        fig.update_layout(
                            uniformtext_minsize=12,
                            uniformtext_mode='hide'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    col_index += 1
        with subtab_cross:
            for key, cross_tab in analysis_results['cross_tabs'].items():
                st.markdown(f"##### {key}")
                st.write(cross_tab)
    
    # 2. 2æŠè³ªå•åˆ†æã‚¿ãƒ–
    with tab_yes_no:
        st.markdown("### 2. 2æŠè³ªå•åˆ†æ")
        
        # å›ç­”åˆ†å¸ƒã®è¡¨ç¤º
        st.markdown("#### å›ç­”åˆ†å¸ƒ")
        for question, dist in analysis_results['response_dist'].items():
            if question in MULTIPLE_CHOICE_QUESTIONS:
                # è¤‡æ•°å›ç­”ã®è³ªå•ã¯é †ä½ã”ã¨ã®å††ã‚°ãƒ©ãƒ•
                st.markdown(f"##### {question}")
                for rank, rank_dist in dist.items():
                    st.markdown(f"###### {rank}")
                    fig = px.pie(
                        values=rank_dist.values,
                        names=rank_dist.index,
                        width=400,
                        height=400
                    )
                    fig.update_layout(
                        uniformtext_minsize=12,
                        uniformtext_mode='hide'
                    )
                    st.plotly_chart(fig, use_container_width=True)
            else:
                # é€šå¸¸ã®2æŠè³ªå•
                fig = px.pie(
                    values=dist.values,
                    names=dist.index,
                    title=question,
                    width=400,
                    height=400
                )
                fig.update_layout(
                    uniformtext_minsize=12,
                    uniformtext_mode='hide'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # å‚¾å‘åˆ†æã®è¡¨ç¤º
        st.markdown("#### å‚¾å‘åˆ†æ")
        for key, trend in analysis_results['trend_analysis'].items():
            st.markdown(f"##### {key}")
            fig = px.imshow(
                trend,
                text_auto=".2f",
                aspect="auto"
            )
            st.plotly_chart(fig)
        
        # ã‚«ã‚¤äºŒä¹—æ¤œå®šçµæœã®è¡¨ç¤º
        st.markdown("#### çµ±è¨ˆçš„æœ‰æ„å·®ã®æ¤œå®š")
        for key, result in analysis_results['chi2_results'].items():
            st.markdown(f"##### {key}")
            st.write(f"ã‚«ã‚¤äºŒä¹—å€¤: {result['chi2']:.2f}")
            st.write(f"på€¤: {result['p_value']:.4f}")
            st.write(f"è‡ªç”±åº¦: {result['dof']}")
    
    # 3. è‡ªç”±è¨˜è¿°åˆ†æã‚¿ãƒ–
    with tab_text:
        st.markdown("### 3. è‡ªç”±è¨˜è¿°åˆ†æ")
        
        for column, analysis in analysis_results['text_analysis'].items():
            st.markdown(f"#### {column}")
            
            # ãƒ†ãƒ¼ãƒã®è¡¨ç¤º
            st.markdown("##### ä¸»è¦ãƒ†ãƒ¼ãƒ")
            for theme in analysis['themes']:
                st.write(f"- {theme}")
            
            # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®è¡¨ç¤º
            st.markdown("##### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
            plt.figure(figsize=(10, 5))
            plt.imshow(analysis['wordcloud'], interpolation='bilinear')
            plt.axis('off')
            st.pyplot(plt)
    
    # 4. ç·åˆåˆ†æã‚¿ãƒ–
    with tab_summary:
        st.markdown("### 4. ç·åˆåˆ†æ")
        
        # ä¸»æˆåˆ†åˆ†æ
        st.markdown("#### ä¸»æˆåˆ†åˆ†æ")
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                title="ä¸»æˆåˆ†åˆ†æçµæœ"
            )
            st.plotly_chart(fig)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        st.markdown("#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
        if len(numeric_columns) > 0:
            kmeans = KMeans(n_clusters=3)
            clusters = kmeans.fit_predict(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                color=clusters,
                title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœ"
            )
            st.plotly_chart(fig)    