import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import chi2_contingency
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import re
from collections import Counter
import itertools
import os
from anthropic import Anthropic

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Anthropic APIã‚­ãƒ¼ã®è¨­å®š
try:
    api_key = st.secrets["ANTHROPIC_API_KEY"]
    
    # Anthropicãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import anthropic
    
    # æ–°ã—ã„APIå½¢å¼ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = anthropic.Anthropic(api_key=api_key)
    
except Exception as e:
    st.error(f"Anthropic APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    st.stop()

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic']

def analyze_free_text_with_anthropic(text_series):
    """Anthropic APIã‚’ä½¿ç”¨ã—ãŸè‡ªç”±è¨˜è¿°ã®åˆ†æï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›å½¢å¼ï¼‰"""
    results = []
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦åˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    combined_text = ' '.join(text_series.dropna())
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt_template = """
    ä»¥ä¸‹ã®è‡ªç”±è¨˜è¿°ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚å›ç­”è€…ã®æ„è¦‹ã€æ„Ÿæƒ³ã€ææ¡ˆã‚’ä½“ç³»çš„ã«æ•´ç†ã—ã€ä»¥ä¸‹ã®å½¢å¼ã§çµæœã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

    ã€åˆ†æå¯¾è±¡ã€‘
    {text}

    ã€åˆ†æçµæœã€‘
    1. ä¸»è¦ãƒ†ãƒ¼ãƒï¼ˆ5ã¤ã¾ã§ã€é‡è¦åº¦é †ï¼‰:
       - ãƒ†ãƒ¼ãƒ1: [ãƒ†ãƒ¼ãƒå] - [ç°¡æ½”ãªèª¬æ˜]
       - ãƒ†ãƒ¼ãƒ2: [ãƒ†ãƒ¼ãƒå] - [ç°¡æ½”ãªèª¬æ˜]
       ...

    2. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ10å€‹ã¾ã§ã€å‡ºç¾é »åº¦é †ï¼‰:
       - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1: [å‡ºç¾å›æ•°]
       - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2: [å‡ºç¾å›æ•°]
       ...

    3. æ„Ÿæƒ…åˆ†æ:
       - ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„è¦‹: [å‰²åˆ]%
       - ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„è¦‹: [å‰²åˆ]%
       - ä¸­ç«‹çš„ãªæ„è¦‹: [å‰²åˆ]%
       - ä¸»ãªãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒ³ãƒˆ: [ç°¡æ½”ã«ç®‡æ¡æ›¸ã]
       - ä¸»ãªãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚¤ãƒ³ãƒˆ: [ç°¡æ½”ã«ç®‡æ¡æ›¸ã]

    4. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®æ„è¦‹ï¼ˆé‡è¦ãªé †ã«5ã¤ã¾ã§ï¼‰:
       - ã‚«ãƒ†ã‚´ãƒªãƒ¼1: 
         * ä¸»ãªæ„è¦‹: [ç°¡æ½”ã«è¦ç´„]
         * å…·ä½“çš„ãªææ¡ˆ: [ã‚ã‚Œã°è¨˜è¼‰]
       - ã‚«ãƒ†ã‚´ãƒªãƒ¼2:
         * ä¸»ãªæ„è¦‹: [ç°¡æ½”ã«è¦ç´„]
         * å…·ä½“çš„ãªææ¡ˆ: [ã‚ã‚Œã°è¨˜è¼‰]
       ...

    5. ç‰¹ç­†ã™ã¹ãå°‘æ•°æ„è¦‹ï¼ˆ3ã¤ã¾ã§ï¼‰:
       - [æ„è¦‹1ã®è¦ç´„]
       - [æ„è¦‹2ã®è¦ç´„]
       - [æ„è¦‹3ã®è¦ç´„]

    6. ç·åˆåˆ†æï¼ˆ200å­—ä»¥å†…ï¼‰:
       [ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå…¨ä½“ã®å‚¾å‘ã€ä¸»ãªç™ºè¦‹ã€ç¤ºå”†ã•ã‚Œã‚‹å¯¾å¿œç­–ãªã©ã‚’ç°¡æ½”ã«è¨˜è¼‰]

    7. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆå„ªå…ˆåº¦é †ã«3ã¤ã¾ã§ï¼‰:
       - [å…·ä½“çš„ãªè¡Œå‹•ææ¡ˆ1]
       - [å…·ä½“çš„ãªè¡Œå‹•ææ¡ˆ2]
       - [å…·ä½“çš„ãªè¡Œå‹•ææ¡ˆ3]
    """
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚€
    formatted_prompt = prompt_template.format(text=combined_text)
    
    try:
        # Anthropic APIã‚’ä½¿ç”¨ã—ã¦åˆ†æã‚’å®Ÿè¡Œ
        message = client.messages.create(
            model="claude-3-5-haiku-latest",
            max_tokens=2000,  # ååˆ†ãªå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ç¢ºä¿
            messages=[
                {
                    "role": "user",
                    "content": formatted_prompt
                }
            ]
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
        if message and hasattr(message, 'content') and message.content:
            analysis_result = message.content[0].text
            results.append(analysis_result)
        else:
            results.append("åˆ†æçµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        
    except Exception as e:
        st.error(f"Anthropic APIã‚’ä½¿ç”¨ã—ãŸåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        results.append(f"åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return results

def analyze_free_text(df, text_columns):
    """è‡ªç”±è¨˜è¿°ã®åˆ†æï¼ˆAnthropic APIã®ã¿ï¼‰"""
    results = {}
    
    for column in text_columns:
        # Anthropic APIã‚’ä½¿ç”¨ã—ãŸåˆ†æ
        anthropic_analysis = analyze_free_text_with_anthropic(df[column])
        
        results[column] = {
            'anthropic_analysis': anthropic_analysis
        }
    
    return results

def process_ranked_attributes(df, question):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ç”¨ï¼šé †ä½ä»˜ãè¤‡æ•°å›ç­”ã®å‡¦ç†ï¼ˆå…¨ä½“åˆ†å¸ƒã‚‚è¿”ã™ï¼‰"""
    answers = []
    for response in df[question]:
        if pd.isna(response):
            continue
        items = [item.strip() for item in str(response).split('ã€')]
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
            # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ä»˜ãã§å‡¦ç†
            for rank, item in enumerate(items, 1):
                answers.append((item, rank))
        elif "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
            # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯é †ä½ãªã—ã§å‡¦ç†
            for item in items:
                answers.append((item, None))
    
    if not answers:
        return {}
    
    result_df = pd.DataFrame(answers, columns=['å›ç­”', 'é †ä½'])
    rank_distributions = {}
    
    if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
        # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã®å ´åˆã€é †ä½ã”ã¨ã®åˆ†å¸ƒã‚’è¨ˆç®—
        max_rank = result_df['é †ä½'].max()
        for rank in range(1, max_rank + 1):
            rank_answers = result_df[result_df['é †ä½'] == rank]['å›ç­”']
            if not rank_answers.empty:
                rank_distributions[f"{rank}ä½"] = rank_answers.value_counts()
    else:
        # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã®å ´åˆã€å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¨ˆç®—
        all_counts = result_df['å›ç­”'].value_counts()
        rank_distributions['å…¨ä½“'] = all_counts
    
    return rank_distributions

def analyze_attributes(df, attributes):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æï¼ˆé€šå¸¸å±æ€§ï¼‹è¤‡æ•°å›ç­”å±æ€§ã®é †ä½åˆ†å¸ƒï¼‰"""
    stats = {}
    ranked_distributions = {}
    for attr in attributes:
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in attr:
            # è¤‡æ•°å›ç­”å±æ€§ã¯é †ä½ã”ã¨ã®åˆ†å¸ƒã‚’è¨ˆç®—
            ranked_distributions[attr] = process_ranked_attributes(df, attr)
        else:
            value_counts = df[attr].value_counts()
            stats[attr] = {
                'count': df[attr].count(),
                'unique': df[attr].nunique(),
                'top': value_counts.index[0] if not value_counts.empty else None,
                'freq': value_counts.iloc[0] if not value_counts.empty else 0,
                'distribution': value_counts.to_dict()
            }
    return stats, ranked_distributions

def analyze_yes_no_questions(df, yes_no_questions, attributes):
    """2æŠè³ªå•ã®åˆ†æ"""
    # 1. å›ç­”åˆ†å¸ƒã®é›†è¨ˆ
    response_dist = {}
    for question in yes_no_questions:
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
            # è¤‡æ•°å›ç­”ã®è³ªå•ã¯ç‰¹åˆ¥å‡¦ç†
            ranked_answers = process_ranked_attributes(df, question)
            if not ranked_answers:
                response_dist[question] = {}
            else:
                response_dist[question] = ranked_answers
        else:
            # é€šå¸¸ã®2æŠè³ªå•
            response_dist[question] = df[question].value_counts(normalize=True)
    
    return response_dist

def visualize_analysis(df, attributes, yes_no_questions, text_columns):
    """åˆ†æçµæœã®å¯è¦–åŒ–"""
    # å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
    stats, attribute_ranked = analyze_attributes(df, attributes)
    # 2æŠè³ªå•ã®åˆ†æ
    response_dist = analyze_yes_no_questions(df, yes_no_questions, attributes)
    # è‡ªç”±è¨˜è¿°ã®åˆ†æ
    text_analysis = analyze_free_text(df, text_columns)
    return {
        'stats': stats,
        'attribute_ranked': attribute_ranked,
        'response_dist': response_dist,
        'text_analysis': text_analysis
    }

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

if uploaded_file is not None:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)
    
    # è³ªå•ã®åˆ†é¡
    attributes = [col for col in df.columns if 'â–ªï¸' in col]
    yes_no_questions = [col for col in df.columns if 'ãƒ»' in col]
    text_columns = [col for col in df.columns if '#' in col]
    
    # åˆ†æã®å®Ÿè¡Œ
    analysis_results = visualize_analysis(df, attributes, yes_no_questions, text_columns)
    
    # ã‚¿ãƒ–ã®ä½œæˆ
    tab_attributes, tab_yes_no, tab_text, tab_summary, tab_cross = st.tabs([
        "1. å±æ€§åˆ†æ",
        "2. 2æŠè³ªå•åˆ†æ",
        "3. è‡ªç”±è¨˜è¿°åˆ†æ",
        "4. ç·åˆåˆ†æ",
        "5. ã‚¯ãƒ­ã‚¹åˆ†æ"
    ])
    
    # 1. å±æ€§åˆ†æã‚¿ãƒ–
    with tab_attributes:
        st.markdown("### 1. å±æ€§åˆ†æ")
        # 3åˆ—ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ
        cols = st.columns(3)
        col_index = 0
        
        for attr in attributes:
            if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in attr:
                # è¤‡æ•°å›ç­”ã®è³ªå•ã¯è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†
                rank_distributions = analysis_results['attribute_ranked'].get(attr, {})
                
                if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr:
                    # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ã”ã¨ã«ç‹¬ç«‹ã—ãŸå›³ã‚’è¡¨ç¤º
                    rank_keys = [k for k in rank_distributions.keys() if k != 'å…¨ä½“']
                    for rank in sorted(rank_keys, key=lambda x: int(x.replace('ä½','')) if x.endswith('ä½') else 999):
                        with cols[col_index % 3]:
                            rank_dist = rank_distributions[rank]
                            # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                            sorted_dist = rank_dist.sort_values(ascending=False)
                            st.markdown(f"###### {attr} - {rank}")
                            fig = px.pie(
                                values=sorted_dist.values,
                                names=sorted_dist.index,
                                width=400,
                                height=400
                            )
                            fig.update_layout(
                                uniformtext_minsize=12,
                                uniformtext_mode='hide'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        col_index += 1
                else:
                    # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¡¨ç¤º
                    with cols[col_index % 3]:
                        all_dist = rank_distributions.get('å…¨ä½“', pd.Series())
                        # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                        sorted_dist = all_dist.sort_values(ascending=False)
                        st.markdown(f"###### {attr}")
                        fig = px.pie(
                            values=sorted_dist.values,
                            names=sorted_dist.index,
                            width=400,
                            height=400
                        )
                        fig.update_layout(
                            uniformtext_minsize=12,
                            uniformtext_mode='hide'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    col_index += 1
            else:
                # é€šå¸¸ã®å±æ€§ã¯1ã¤ã®å›³ã‚’è¡¨ç¤º
                with cols[col_index % 3]:
                    stat = analysis_results['stats'][attr]
                    # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_dist = pd.Series(stat['distribution']).sort_values(ascending=False)
                    st.markdown(f"###### {attr}")
                    fig = px.pie(
                        values=sorted_dist.values,
                        names=sorted_dist.index,
                        width=400,
                        height=400
                    )
                    fig.update_layout(
                        uniformtext_minsize=12,
                        uniformtext_mode='hide'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                col_index += 1
    
    # 2. 2æŠè³ªå•åˆ†æã‚¿ãƒ–
    with tab_yes_no:
        st.markdown("### 2. 2æŠè³ªå•åˆ†æ")
        
        # å›ç­”åˆ†å¸ƒã®è¡¨ç¤º
        st.markdown("#### å›ç­”åˆ†å¸ƒ")
        # 3åˆ—ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ
        cols = st.columns(3)
        col_index = 0
        
        for question, dist in analysis_results['response_dist'].items():
            if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
                # è¤‡æ•°å›ç­”ã®è³ªå•ã¯è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†
                if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
                    # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ã”ã¨ã®å††ã‚°ãƒ©ãƒ•
                    for rank, rank_dist in dist.items():
                        with cols[col_index % 3]:
                            # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                            sorted_dist = rank_dist.sort_values(ascending=False)
                            st.markdown(f"###### {question} - {rank}")
                            fig = px.pie(
                                values=sorted_dist.values,
                                names=sorted_dist.index,
                                width=400,
                                height=400
                            )
                            fig.update_layout(
                                uniformtext_minsize=12,
                                uniformtext_mode='hide'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        col_index += 1
                else:
                    # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¡¨ç¤º
                    with cols[col_index % 3]:
                        all_dist = dist.get('å…¨ä½“', pd.Series())
                        # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                        sorted_dist = all_dist.sort_values(ascending=False)
                        st.markdown(f"###### {question}")
                        fig = px.pie(
                            values=sorted_dist.values,
                            names=sorted_dist.index,
                            width=400,
                            height=400
                        )
                        fig.update_layout(
                            uniformtext_minsize=12,
                            uniformtext_mode='hide'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    col_index += 1
            else:
                # é€šå¸¸ã®2æŠè³ªå•
                with cols[col_index % 3]:
                    # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_dist = dist.sort_values(ascending=False)
                    st.markdown(f"###### {question}")
                    fig = px.pie(
                        values=sorted_dist.values,
                        names=sorted_dist.index,
                        width=400,
                        height=400
                    )
                    fig.update_layout(
                        uniformtext_minsize=12,
                        uniformtext_mode='hide'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                col_index += 1
    
    # 3. è‡ªç”±è¨˜è¿°åˆ†æã‚¿ãƒ–
    with tab_text:
        st.markdown("### 3. è‡ªç”±è¨˜è¿°åˆ†æ")
        
        # è³ªå•ã”ã¨ã®ã‚µãƒ–ã‚¿ãƒ–ã‚’ä½œæˆ
        if analysis_results['text_analysis']:
            # ã‚µãƒ–ã‚¿ãƒ–ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            text_tabs = st.tabs([f"è³ªå•{i+1}: {column.split('#')[1] if '#' in column else column}" 
                               for i, column in enumerate(analysis_results['text_analysis'].keys())])
            
            # å„ã‚µãƒ–ã‚¿ãƒ–ã«åˆ†æçµæœã‚’è¡¨ç¤º
            for tab, (column, analysis) in zip(text_tabs, analysis_results['text_analysis'].items()):
                with tab:
                    # å…ƒã®è³ªå•æ–‡ã‚’è¡¨ç¤º
                    st.markdown(f"**è³ªå•æ–‡**: {column}")
                    st.markdown("---")
                    
                    # Anthropicã«ã‚ˆã‚‹åˆ†æçµæœã®è¡¨ç¤º
                    st.markdown("#### AIåˆ†æçµæœ")
                    for result in analysis['anthropic_analysis']:
                        st.markdown(result)
                        
    # 4. ç·åˆåˆ†æã‚¿ãƒ–
    with tab_summary:
        st.markdown("### 4. ç·åˆåˆ†æ")
        
        # ä¸»æˆåˆ†åˆ†æ
        st.markdown("#### ä¸»æˆåˆ†åˆ†æ")
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                title="ä¸»æˆåˆ†åˆ†æçµæœ"
            )
            st.plotly_chart(fig)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        st.markdown("#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
        if len(numeric_columns) > 0:
            kmeans = KMeans(n_clusters=3)
            clusters = kmeans.fit_predict(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                color=clusters,
                title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœ"
            )
            st.plotly_chart(fig)

    # 5. ã‚¯ãƒ­ã‚¹åˆ†æã‚¿ãƒ–
    with tab_cross:
        def display_cross_analysis(df):
            st.markdown("### 5. ã‚¯ãƒ­ã‚¹åˆ†æ")

            tabs = st.tabs([
                "æ€§åˆ¥ Ã— èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ",
                "æ€§åˆ¥ Ã— èˆˆå‘³ã®ã‚ã‚‹è·ç¨®",
                "å­¦å¹´ Ã— èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ",
                "å­¦å¹´ Ã— èˆˆå‘³ã®ã‚ã‚‹è·ç¨®",
                "å‡ºèº«åœ° Ã— èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ",
                "å‡ºèº«åœ° Ã— èˆˆå‘³ã®ã‚ã‚‹è·ç¨®",
                "å­¦å¹´ Ã— æ†§ã‚Œã¦ã„ã‚‹æ¥­ç•Œ"
            ])

            # å®šç¾©: å„ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚¿ãƒ–ã®æƒ…å ±
            cross_info = [
                ("â–ªï¸ æ€§åˆ¥", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ"),
                ("â–ªï¸ æ€§åˆ¥", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹è·ç¨®"),
                ("â–ªï¸ å­¦å¹´", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ"),
                ("â–ªï¸ å­¦å¹´", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹è·ç¨®"),
                ("â–ªï¸ å‡ºèº«åœ°", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹æ¥­ç•Œ"),
                ("â–ªï¸ å‡ºèº«åœ°", "â–ªï¸ èˆˆå‘³ã®ã‚ã‚‹è·ç¨®"),
                ("â–ªï¸ å­¦å¹´", "â–ªï¸ æ†§ã‚Œã¦ã„ã‚‹æ¥­ç•Œ")
            ]

            for tab, (row_attr, col_attr) in zip(tabs, cross_info):
                with tab:
                    st.markdown(f"#### {row_attr} Ã— {col_attr} ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ")
                    try:
                        ct = pd.crosstab(df[row_attr], df[col_attr])
                        ct = ct.loc[:, ct.sum().sort_values(ascending=False).index]  # ã‚«ãƒ©ãƒ ã‚’é »åº¦é †ã«

                        fig = px.bar(
                            ct.T,
                            barmode="stack",
                            title=f"{row_attr} Ã— {col_attr}"
                        )
                        fig.update_layout(xaxis_title=col_attr, yaxis_title="äººæ•°")
                        st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.warning(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        # ã‚¯ãƒ­ã‚¹åˆ†æã®å®Ÿè¡Œ
        display_cross_analysis(df)    