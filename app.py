import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import chi2_contingency
import networkx as nx
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import japanize_matplotlib
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import re
from collections import Counter
import itertools

# OpenAI APIã‚­ãƒ¼ã®è¨­å®š
try:
    openai.api_key = st.secrets["OPENAI_API_KEY"]
except Exception as e:
    st.error(f"OpenAI APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    st.stop()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

def preprocess_text(text_series):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
    # NaNã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    text_series = text_series.fillna('')
    
    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
    text_series = text_series.str.translate(str.maketrans(
        'ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
    ))
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text_series = text_series.str.replace(r'[^\w\s]', '', regex=True)
    
    return text_series

def extract_themes(text_series, n_themes=5):
    """ãƒ†ãƒ¼ãƒã®æŠ½å‡º"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = ' '.join(text_series).split()
    
    # å˜èªã®å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(words)
    
    # ä¸Šä½n_themeså€‹ã®ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    themes = [word for word, count in word_counts.most_common(n_themes)]
    
    return themes

def build_co_occurrence_network(text_series, window_size=2):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = ' '.join(text_series).split()
    
    # å…±èµ·é–¢ä¿‚ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    co_occurrence = {}
    for i in range(len(words)):
        for j in range(i+1, min(i+window_size+1, len(words))):
            pair = tuple(sorted([words[i], words[j]]))
            co_occurrence[pair] = co_occurrence.get(pair, 0) + 1
    
    return co_occurrence

def analyze_attributes(df, attributes):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
    # 1. åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—
    stats = df[attributes].describe()
    
    # 2. ã‚¯ãƒ­ã‚¹é›†è¨ˆ
    cross_tabs = {}
    for attr1, attr2 in itertools.combinations(attributes, 2):
        cross_tabs[f"{attr1}_vs_{attr2}"] = pd.crosstab(df[attr1], df[attr2])
    
    # 3. ç›¸é–¢åˆ†æ
    correlation = df[attributes].corr()
    
    return stats, cross_tabs, correlation

def analyze_yes_no_questions(df, yes_no_questions, attributes):
    """2æŠè³ªå•ã®åˆ†æ"""
    # 1. å›ç­”åˆ†å¸ƒã®é›†è¨ˆ
    response_dist = {}
    for question in yes_no_questions:
        response_dist[question] = df[question].value_counts(normalize=True)
    
    # 2. å±æ€§åˆ¥ã®å‚¾å‘åˆ†æ
    trend_analysis = {}
    for question in yes_no_questions:
        for attribute in attributes:
            trend_analysis[f"{question}_by_{attribute}"] = pd.crosstab(
                df[attribute], 
                df[question], 
                normalize='index'
            )
    
    # 3. ã‚«ã‚¤äºŒä¹—æ¤œå®š
    chi2_results = {}
    for question in yes_no_questions:
        for attribute in attributes:
            contingency_table = pd.crosstab(df[attribute], df[question])
            chi2, p, dof, expected = chi2_contingency(contingency_table)
            chi2_results[f"{question}_vs_{attribute}"] = {
                'chi2': chi2,
                'p_value': p,
                'dof': dof
            }
    
    return response_dist, trend_analysis, chi2_results

def analyze_free_text(df, text_columns):
    """è‡ªç”±è¨˜è¿°ã®åˆ†æ"""
    results = {}
    
    for column in text_columns:
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_text = preprocess_text(df[column])
        
        # ãƒ†ãƒ¼ãƒã®æŠ½å‡º
        themes = extract_themes(processed_text)
        
        # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰
        co_occurrence = build_co_occurrence_network(processed_text)
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            font_path=None  # ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
        ).generate(' '.join(processed_text))
        
        results[column] = {
            'themes': themes,
            'co_occurrence': co_occurrence,
            'wordcloud': wordcloud
        }
    
    return results

def visualize_analysis(df, attributes, yes_no_questions, text_columns):
    """åˆ†æçµæœã®å¯è¦–åŒ–"""
    # 1. å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
    stats, cross_tabs, correlation = analyze_attributes(df, attributes)
    
    # 2. 2æŠè³ªå•ã®åˆ†æ
    response_dist, trend_analysis, chi2_results = analyze_yes_no_questions(
        df, yes_no_questions, attributes
    )
    
    # 3. è‡ªç”±è¨˜è¿°ã®åˆ†æ
    text_analysis = analyze_free_text(df, text_columns)
    
    return {
        'stats': stats,
        'cross_tabs': cross_tabs,
        'correlation': correlation,
        'response_dist': response_dist,
        'trend_analysis': trend_analysis,
        'chi2_results': chi2_results,
        'text_analysis': text_analysis
    }

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

if uploaded_file is not None:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)
    
    # è³ªå•ã®åˆ†é¡
    attributes = [col for col in df.columns if 'â–ªï¸' in col]
    yes_no_questions = [col for col in df.columns if 'âš«ï¸' in col]
    text_columns = [col for col in df.columns if 'ãƒ»' in col]
    
    # åˆ†æã®å®Ÿè¡Œ
    analysis_results = visualize_analysis(df, attributes, yes_no_questions, text_columns)
    
    # ã‚¿ãƒ–ã®ä½œæˆ
    tab_attributes, tab_yes_no, tab_text, tab_summary = st.tabs([
        "1. å±æ€§åˆ†æ",
        "2. 2æŠè³ªå•åˆ†æ",
        "3. è‡ªç”±è¨˜è¿°åˆ†æ",
        "4. ç·åˆåˆ†æ"
    ])
    
    # 1. å±æ€§åˆ†æã‚¿ãƒ–
    with tab_attributes:
        st.markdown("### 1. å±æ€§åˆ†æ")
        
        # åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
        st.markdown("#### åŸºæœ¬çµ±è¨ˆé‡")
        st.write(analysis_results['stats'])
        
        # ç›¸é–¢åˆ†æã®è¡¨ç¤º
        st.markdown("#### ç›¸é–¢åˆ†æ")
        fig = px.imshow(
            analysis_results['correlation'],
            text_auto=".2f",
            aspect="auto"
        )
        st.plotly_chart(fig)
        
        # ã‚¯ãƒ­ã‚¹é›†è¨ˆã®è¡¨ç¤º
        st.markdown("#### ã‚¯ãƒ­ã‚¹é›†è¨ˆ")
        for key, cross_tab in analysis_results['cross_tabs'].items():
            st.markdown(f"##### {key}")
            st.write(cross_tab)
    
    # 2. 2æŠè³ªå•åˆ†æã‚¿ãƒ–
    with tab_yes_no:
        st.markdown("### 2. 2æŠè³ªå•åˆ†æ")
        
        # å›ç­”åˆ†å¸ƒã®è¡¨ç¤º
        st.markdown("#### å›ç­”åˆ†å¸ƒ")
        for question, dist in analysis_results['response_dist'].items():
            fig = px.pie(
                values=dist.values,
                names=dist.index,
                title=question
            )
            st.plotly_chart(fig)
        
        # å‚¾å‘åˆ†æã®è¡¨ç¤º
        st.markdown("#### å‚¾å‘åˆ†æ")
        for key, trend in analysis_results['trend_analysis'].items():
            st.markdown(f"##### {key}")
            fig = px.imshow(
                trend,
                text_auto=".2f",
                aspect="auto"
            )
            st.plotly_chart(fig)
        
        # ã‚«ã‚¤äºŒä¹—æ¤œå®šçµæœã®è¡¨ç¤º
        st.markdown("#### çµ±è¨ˆçš„æœ‰æ„å·®ã®æ¤œå®š")
        for key, result in analysis_results['chi2_results'].items():
            st.markdown(f"##### {key}")
            st.write(f"ã‚«ã‚¤äºŒä¹—å€¤: {result['chi2']:.2f}")
            st.write(f"på€¤: {result['p_value']:.4f}")
            st.write(f"è‡ªç”±åº¦: {result['dof']}")
    
    # 3. è‡ªç”±è¨˜è¿°åˆ†æã‚¿ãƒ–
    with tab_text:
        st.markdown("### 3. è‡ªç”±è¨˜è¿°åˆ†æ")
        
        for column, analysis in analysis_results['text_analysis'].items():
            st.markdown(f"#### {column}")
            
            # ãƒ†ãƒ¼ãƒã®è¡¨ç¤º
            st.markdown("##### ä¸»è¦ãƒ†ãƒ¼ãƒ")
            for theme in analysis['themes']:
                st.write(f"- {theme}")
            
            # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®è¡¨ç¤º
            st.markdown("##### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
            plt.figure(figsize=(10, 5))
            plt.imshow(analysis['wordcloud'], interpolation='bilinear')
            plt.axis('off')
            st.pyplot(plt)
    
    # 4. ç·åˆåˆ†æã‚¿ãƒ–
    with tab_summary:
        st.markdown("### 4. ç·åˆåˆ†æ")
        
        # ä¸»æˆåˆ†åˆ†æ
        st.markdown("#### ä¸»æˆåˆ†åˆ†æ")
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                title="ä¸»æˆåˆ†åˆ†æçµæœ"
            )
            st.plotly_chart(fig)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        st.markdown("#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
        if len(numeric_columns) > 0:
            kmeans = KMeans(n_clusters=3)
            clusters = kmeans.fit_predict(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                color=clusters,
                title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœ"
            )
            st.plotly_chart(fig)    