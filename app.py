import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import chi2_contingency
import networkx as nx
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import re
from collections import Counter
import itertools
import os
from janome.tokenizer import Tokenizer
import unicodedata
import urllib.request
import tempfile
# import openai

# # OpenAI APIã‚­ãƒ¼ã®è¨­å®š
# try:
#     openai.api_key = st.secrets["OPENAI_API_KEY"]
# except Exception as e:
#     st.error(f"OpenAI APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
#     st.stop()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic']

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è¨­å®š
@st.cache_resource
def get_japanese_font():
    try:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        temp_dir = tempfile.mkdtemp()
        font_path = os.path.join(temp_dir, 'NotoSansCJKjp-Regular.otf')
        
        # Google Fontsã‹ã‚‰Noto Sans CJK JPã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        font_url = 'https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf'
        urllib.request.urlretrieve(font_url, font_path)
        
        # matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’æ›´æ–°
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']
        plt.rcParams['axes.unicode_minus'] = False
        
        return font_path
    except Exception as e:
        st.error(f"ãƒ•ã‚©ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

# ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®å–å¾—
FONT_PATH = get_japanese_font()

# janomeã®åˆæœŸåŒ–
tokenizer = Tokenizer()

def preprocess_text(text_series):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
    # NaNã‚’ç©ºæ–‡å­—åˆ—ã«å¤‰æ›
    text_series = text_series.fillna('')
    
    # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
    text_series = text_series.str.translate(str.maketrans(
        'ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
        'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
    ))
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text_series = text_series.str.replace(r'[^\w\s]', '', regex=True)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–
    text_series = text_series.apply(lambda x: unicodedata.normalize('NFKC', x))
    
    return text_series

def extract_themes(text_series, n_themes=5):
    """ãƒ†ãƒ¼ãƒã®æŠ½å‡º"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = []
    for text in text_series:
        if text:
            # janomeã§å˜èªåˆ†å‰²
            tokens = tokenizer.tokenize(text)
            # åŠ©è©ã‚’é™¤å¤–ã—ã¦å˜èªã‚’æŠ½å‡º
            words.extend([
                token.surface for token in tokens 
                if token.part_of_speech.split(',')[0] not in ['åŠ©è©', 'åŠ©å‹•è©', 'æ¥ç¶šè©', 'è¨˜å·']
            ])
    
    # å˜èªã®å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(words)
    
    # ä¸Šä½n_themeså€‹ã®ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    themes = [word for word, count in word_counts.most_common(n_themes)]
    
    return themes

def build_co_occurrence_network(text_series, window_size=2):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰"""
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = []
    for text in text_series:
        if text:
            # janomeã§å˜èªåˆ†å‰²
            tokens = tokenizer.tokenize(text)
            # åŠ©è©ã‚’é™¤å¤–ã—ã¦å˜èªã‚’æŠ½å‡º
            words.extend([
                token.surface for token in tokens 
                if token.part_of_speech.split(',')[0] not in ['åŠ©è©', 'åŠ©å‹•è©', 'æ¥ç¶šè©', 'è¨˜å·']
            ])
    
    # å…±èµ·é–¢ä¿‚ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    co_occurrence = {}
    for i in range(len(words)):
        for j in range(i+1, min(i+window_size+1, len(words))):
            pair = tuple(sorted([words[i], words[j]]))
            co_occurrence[pair] = co_occurrence.get(pair, 0) + 1
    
    return co_occurrence

def process_ranked_attributes(df, question):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ç”¨ï¼šé †ä½ä»˜ãè¤‡æ•°å›ç­”ã®å‡¦ç†ï¼ˆå…¨ä½“åˆ†å¸ƒã‚‚è¿”ã™ï¼‰"""
    answers = []
    for response in df[question]:
        if pd.isna(response):
            continue
        items = [item.strip() for item in str(response).split('ã€')]
        
        # è³ªå•ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
            # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ä»˜ãã§å‡¦ç†
            for rank, item in enumerate(items, 1):
                answers.append((item, rank))
        elif "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
            # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯é †ä½ãªã—ã§å‡¦ç†
            for item in items:
                answers.append((item, None))
    
    if not answers:
        return {}
    
    result_df = pd.DataFrame(answers, columns=['å›ç­”', 'é †ä½'])
    rank_distributions = {}
    
    if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
        # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã®å ´åˆã€é †ä½ã”ã¨ã®åˆ†å¸ƒã‚’è¨ˆç®—
        max_rank = result_df['é †ä½'].max()
        for rank in range(1, max_rank + 1):
            rank_answers = result_df[result_df['é †ä½'] == rank]['å›ç­”']
            if not rank_answers.empty:
                rank_distributions[f"{rank}ä½"] = rank_answers.value_counts()
    else:
        # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã®å ´åˆã€å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¨ˆç®—
        all_counts = result_df['å›ç­”'].value_counts()
        rank_distributions['å…¨ä½“'] = all_counts
    
    return rank_distributions

def analyze_attributes(df, attributes):
    """å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æï¼ˆé€šå¸¸å±æ€§ï¼‹è¤‡æ•°å›ç­”å±æ€§ã®é †ä½åˆ†å¸ƒï¼‰"""
    stats = {}
    ranked_distributions = {}
    for attr in attributes:
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in attr:
            # è¤‡æ•°å›ç­”å±æ€§ã¯é †ä½ã”ã¨ã®åˆ†å¸ƒã‚’è¨ˆç®—
            ranked_distributions[attr] = process_ranked_attributes(df, attr)
        else:
            value_counts = df[attr].value_counts()
            stats[attr] = {
                'count': df[attr].count(),
                'unique': df[attr].nunique(),
                'top': value_counts.index[0] if not value_counts.empty else None,
                'freq': value_counts.iloc[0] if not value_counts.empty else 0,
                'distribution': value_counts.to_dict()
            }
    return stats, ranked_distributions

def analyze_yes_no_questions(df, yes_no_questions, attributes):
    """2æŠè³ªå•ã®åˆ†æ"""
    # 1. å›ç­”åˆ†å¸ƒã®é›†è¨ˆ
    response_dist = {}
    for question in yes_no_questions:
        if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
            # è¤‡æ•°å›ç­”ã®è³ªå•ã¯ç‰¹åˆ¥å‡¦ç†
            ranked_answers = process_ranked_attributes(df, question)
            if not ranked_answers:
                response_dist[question] = {}
            else:
                response_dist[question] = ranked_answers
        else:
            # é€šå¸¸ã®2æŠè³ªå•
            response_dist[question] = df[question].value_counts(normalize=True)
    
    return response_dist

def analyze_free_text(df, text_columns):
    """è‡ªç”±è¨˜è¿°ã®åˆ†æ"""
    results = {}
    
    for column in text_columns:
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_text = preprocess_text(df[column])
        
        # ãƒ†ãƒ¼ãƒã®æŠ½å‡º
        themes = extract_themes(processed_text)
        
        # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰
        co_occurrence = build_co_occurrence_network(processed_text)
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦janomeã§å‡¦ç†
            combined_text = ' '.join(processed_text)
            tokens = tokenizer.tokenize(combined_text)
            # åŠ©è©ã‚’é™¤å¤–ã—ã¦å˜èªã‚’æŠ½å‡º
            parsed_text = ' '.join([
                token.surface for token in tokens 
                if token.part_of_speech.split(',')[0] not in ['åŠ©è©', 'åŠ©å‹•è©', 'æ¥ç¶šè©', 'è¨˜å·']
            ])
            
            # ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
            if FONT_PATH and os.path.exists(FONT_PATH):
                font_path = FONT_PATH
            else:
                font_path = None
                st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                font_path=font_path,  # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®š
                min_font_size=10,
                max_font_size=100,
                collocations=False,  # æ—¥æœ¬èªã®å ´åˆã¯Falseã«è¨­å®š
                regexp=r"[\w']+",  # å˜èªã®åŒºåˆ‡ã‚Šã‚’èª¿æ•´
                prefer_horizontal=0.8,  # æ¨ªæ›¸ãã®æ¯”ç‡ã‚’èª¿æ•´
                colormap='viridis',  # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’æŒ‡å®š
                relative_scaling=0.5  # å˜èªã®ç›¸å¯¾çš„ãªã‚µã‚¤ã‚ºã‚’èª¿æ•´
            ).generate(parsed_text)
        except Exception as e:
            st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            wordcloud = None
        
        results[column] = {
            'themes': themes,
            'co_occurrence': co_occurrence,
            'wordcloud': wordcloud
        }
    
    return results

def visualize_analysis(df, attributes, yes_no_questions, text_columns):
    """åˆ†æçµæœã®å¯è¦–åŒ–"""
    # å±æ€§ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
    stats, attribute_ranked = analyze_attributes(df, attributes)
    # 2æŠè³ªå•ã®åˆ†æ
    response_dist = analyze_yes_no_questions(df, yes_no_questions, attributes)
    # è‡ªç”±è¨˜è¿°ã®åˆ†æ
    text_analysis = analyze_free_text(df, text_columns)
    return {
        'stats': stats,
        'attribute_ranked': attribute_ranked,
        'response_dist': response_dist,
        'text_analysis': text_analysis
    }

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("æ¡ç”¨å‹•ç”»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœåˆ†æ")

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

if uploaded_file is not None:
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)
    
    # è³ªå•ã®åˆ†é¡
    attributes = [col for col in df.columns if 'â–ªï¸' in col]
    yes_no_questions = [col for col in df.columns if 'ãƒ»' in col]
    text_columns = [col for col in df.columns if '#' in col]
    
    # åˆ†æã®å®Ÿè¡Œ
    analysis_results = visualize_analysis(df, attributes, yes_no_questions, text_columns)
    
    # ã‚¿ãƒ–ã®ä½œæˆ
    tab_attributes, tab_yes_no, tab_text, tab_summary = st.tabs([
        "1. å±æ€§åˆ†æ",
        "2. 2æŠè³ªå•åˆ†æ",
        "3. è‡ªç”±è¨˜è¿°åˆ†æ",
        "4. ç·åˆåˆ†æ"
    ])
    
    # 1. å±æ€§åˆ†æã‚¿ãƒ–
    with tab_attributes:
        st.markdown("### 1. å±æ€§åˆ†æ")
        # 3åˆ—ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ
        cols = st.columns(3)
        col_index = 0
        
        for attr in attributes:
            if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in attr:
                # è¤‡æ•°å›ç­”ã®è³ªå•ã¯è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†
                rank_distributions = analysis_results['attribute_ranked'].get(attr, {})
                
                if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in attr:
                    # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ã”ã¨ã«ç‹¬ç«‹ã—ãŸå›³ã‚’è¡¨ç¤º
                    rank_keys = [k for k in rank_distributions.keys() if k != 'å…¨ä½“']
                    for rank in sorted(rank_keys, key=lambda x: int(x.replace('ä½','')) if x.endswith('ä½') else 999):
                        with cols[col_index % 3]:
                            rank_dist = rank_distributions[rank]
                            # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                            sorted_dist = rank_dist.sort_values(ascending=False)
                            st.markdown(f"###### {attr} - {rank}")
                            fig = px.pie(
                                values=sorted_dist.values,
                                names=sorted_dist.index,
                                width=400,
                                height=400
                            )
                            fig.update_layout(
                                uniformtext_minsize=12,
                                uniformtext_mode='hide'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        col_index += 1
                else:
                    # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¡¨ç¤º
                    with cols[col_index % 3]:
                        all_dist = rank_distributions.get('å…¨ä½“', pd.Series())
                        # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                        sorted_dist = all_dist.sort_values(ascending=False)
                        st.markdown(f"###### {attr}")
                        fig = px.pie(
                            values=sorted_dist.values,
                            names=sorted_dist.index,
                            width=400,
                            height=400
                        )
                        fig.update_layout(
                            uniformtext_minsize=12,
                            uniformtext_mode='hide'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    col_index += 1
            else:
                # é€šå¸¸ã®å±æ€§ã¯1ã¤ã®å›³ã‚’è¡¨ç¤º
                with cols[col_index % 3]:
                    stat = analysis_results['stats'][attr]
                    # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_dist = pd.Series(stat['distribution']).sort_values(ascending=False)
                    st.markdown(f"###### {attr}")
                    fig = px.pie(
                        values=sorted_dist.values,
                        names=sorted_dist.index,
                        width=400,
                        height=400
                    )
                    fig.update_layout(
                        uniformtext_minsize=12,
                        uniformtext_mode='hide'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                col_index += 1
    
    # 2. 2æŠè³ªå•åˆ†æã‚¿ãƒ–
    with tab_yes_no:
        st.markdown("### 2. 2æŠè³ªå•åˆ†æ")
        
        # å›ç­”åˆ†å¸ƒã®è¡¨ç¤º
        st.markdown("#### å›ç­”åˆ†å¸ƒ")
        # 3åˆ—ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ
        cols = st.columns(3)
        col_index = 0
        
        for question, dist in analysis_results['response_dist'].items():
            if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question or "ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰" in question:
                # è¤‡æ•°å›ç­”ã®è³ªå•ã¯è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†
                if "ï¼ˆä¸Šä½3ã¤ã¾ã§ï¼‰" in question:
                    # ä¸Šä½3ã¤ã¾ã§ã®è³ªå•ã¯é †ä½ã”ã¨ã®å††ã‚°ãƒ©ãƒ•
                    for rank, rank_dist in dist.items():
                        with cols[col_index % 3]:
                            # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                            sorted_dist = rank_dist.sort_values(ascending=False)
                            st.markdown(f"###### {question} - {rank}")
                            fig = px.pie(
                                values=sorted_dist.values,
                                names=sorted_dist.index,
                                width=400,
                                height=400
                            )
                            fig.update_layout(
                                uniformtext_minsize=12,
                                uniformtext_mode='hide'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                        col_index += 1
                else:
                    # è¤‡æ•°é¸æŠå¯ã®è³ªå•ã¯å…¨ä½“ã®åˆ†å¸ƒã®ã¿ã‚’è¡¨ç¤º
                    with cols[col_index % 3]:
                        all_dist = dist.get('å…¨ä½“', pd.Series())
                        # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                        sorted_dist = all_dist.sort_values(ascending=False)
                        st.markdown(f"###### {question}")
                        fig = px.pie(
                            values=sorted_dist.values,
                            names=sorted_dist.index,
                            width=400,
                            height=400
                        )
                        fig.update_layout(
                            uniformtext_minsize=12,
                            uniformtext_mode='hide'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    col_index += 1
            else:
                # é€šå¸¸ã®2æŠè³ªå•
                with cols[col_index % 3]:
                    # å›ç­”æ•°ã§ã‚½ãƒ¼ãƒˆ
                    sorted_dist = dist.sort_values(ascending=False)
                    st.markdown(f"###### {question}")
                    fig = px.pie(
                        values=sorted_dist.values,
                        names=sorted_dist.index,
                        width=400,
                        height=400
                    )
                    fig.update_layout(
                        uniformtext_minsize=12,
                        uniformtext_mode='hide'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                col_index += 1
    
    # 3. è‡ªç”±è¨˜è¿°åˆ†æã‚¿ãƒ–
    with tab_text:
        st.markdown("### 3. è‡ªç”±è¨˜è¿°åˆ†æ")
        
        for column, analysis in analysis_results['text_analysis'].items():
            st.markdown(f"#### {column}")
            
            # ãƒ†ãƒ¼ãƒã®è¡¨ç¤º
            st.markdown("##### ä¸»è¦ãƒ†ãƒ¼ãƒ")
            for theme in analysis['themes']:
                st.write(f"- {theme}")
            
            # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®è¡¨ç¤º
            st.markdown("##### ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
            if analysis['wordcloud'] is not None:
                plt.figure(figsize=(10, 5))
                plt.imshow(analysis['wordcloud'], interpolation='bilinear')
                plt.axis('off')
                st.pyplot(plt)
            else:
                st.warning("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    # 4. ç·åˆåˆ†æã‚¿ãƒ–
    with tab_summary:
        st.markdown("### 4. ç·åˆåˆ†æ")
        
        # ä¸»æˆåˆ†åˆ†æ
        st.markdown("#### ä¸»æˆåˆ†åˆ†æ")
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                title="ä¸»æˆåˆ†åˆ†æçµæœ"
            )
            st.plotly_chart(fig)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        st.markdown("#### ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ")
        if len(numeric_columns) > 0:
            kmeans = KMeans(n_clusters=3)
            clusters = kmeans.fit_predict(df[numeric_columns])
            fig = px.scatter(
                x=pca_result[:, 0],
                y=pca_result[:, 1],
                color=clusters,
                title="ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æçµæœ"
            )
            st.plotly_chart(fig)    